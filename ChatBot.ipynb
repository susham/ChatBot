{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.preprocessing.text as t\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data [movie_lines] to extract just the lines and also create a dictionary with dialogue id as key and dialogue as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_conversation_exists = os.path.exists(os.path.join('data', 'dialogue_conversation'))\n",
    "movie_lines_exists = os.path.exists(os.path.join('data', 'movie_lines.txt'))\n",
    "\n",
    "if not (dialogue_conversation_exists & movie_lines_exists):\n",
    "    raw_movie_lines = open(os.path.join('data', 'movie_lines.txt'), 'r').read().split('\\n')[:-1]\n",
    "    dialogue_conversation = {}\n",
    "    \n",
    "    with open(os.path.join('data','just_movie_lines.txt'), 'w') as f:\n",
    "        for line in raw_movie_lines:\n",
    "            line = line.split(' +++$+++ ')\n",
    "            dialogue_id = line[0]\n",
    "            conversation = line[-1]\n",
    "            f.write(conversation + '\\n')\n",
    "            dialogue_conversation[dialogue_id] = conversation\n",
    "    \n",
    "    pickle.dump(dialogue_conversation, open(os.path.join('data', 'dialogue_conversation'), 'wb'), True)\n",
    "else:\n",
    "    dialogue_conversation = pickle.load(open(os.path.join('data', 'dialogue_conversation'), 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Embedding Indices from Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "if not os.path.exists(os.path.join('data', 'embeddings_index')):\n",
    "    f = open(os.path.join('glove.6B', 'glove.6B.100d.txt'), encoding = 'utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeffs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coeffs\n",
    "    f.close()\n",
    "    \n",
    "    pickle.dump(embeddings_index, open(os.path.join('data', 'embeddings_index'), 'wb'), True)\n",
    "else:\n",
    "    embeddings_index = pickle.load(open(os.path.join('data', 'embeddings_index'), 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset to extract words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(os.path.join('data','just_movie_lines.txt'), 'r').read().split('\\n')[:-1]\n",
    "min_count = 15\n",
    "tokenizer = t.Tokenizer(lines)\n",
    "tokenizer.fit_on_texts(lines) \n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Current vocabulary after choosing only most frequent words', '8424')\n"
     ]
    }
   ],
   "source": [
    "print('Current vocabulary after choosing only most frequent words', str(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = t.Tokenizer(num_words=num_words)\n",
    "# Assigns id to words in the lines according to word count\n",
    "tokenizer.fit_on_texts(lines) \n",
    "\n",
    "# word_index is a dictionary of word and its index.\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Index and Index to Word Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {key: word_index[key] + 3 for key in word_index if word_index[key] <= num_words}\n",
    "index_to_word = {word_to_index[key]: key for key in word_to_index}\n",
    "\n",
    "word_to_index['<pad>'] = 0\n",
    "word_to_index['<bos>'] = 1\n",
    "word_to_index['<eos>'] = 2\n",
    "word_to_index['<unk>'] = 3\n",
    "\n",
    "index_to_word[0] = '<pad>'\n",
    "index_to_word[1] = '<bos>'\n",
    "index_to_word[2] = '<eos>'\n",
    "index_to_word[3] = '<unk>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\")\n"
     ]
    }
   ],
   "source": [
    "conversations = []\n",
    "conversations_exists = os.path.exists(os.path.join('data', 'conversations'))\n",
    "\n",
    "if not conversations_exists:\n",
    "    raw_movie_conversations = open(os.path.join('data', 'movie_conversations.txt'), 'r').read().split('\\n')[:-1]\n",
    "    \n",
    "    # Extracting the conversation list and forming a list of conversations \n",
    "    # Here con_a is previous two lines, con_a_2 is current line and con_b is next/target line.\n",
    "    for conversation in raw_movie_conversations:\n",
    "        conversation = conversation.split(' +++$+++ ')[-1]\n",
    "        conversation = conversation.replace('[', '')\n",
    "        conversation = conversation.replace(']', '')\n",
    "        conversation = conversation.replace('\\'', '')\n",
    "        conversation = conversation.split(', ')\n",
    "        \n",
    "        con_a_1 = ''\n",
    "        for i in range(len(conversation)-1):\n",
    "            \n",
    "            con_a_2 = dialogue_conversation[conversation[i]]\n",
    "            con_b = dialogue_conversation[conversation[i+1]]\n",
    "            \n",
    "            if len(con_a_1.split()) <= 50 and len(con_a_2.split()) <= 50 and len(con_b.split()) <= 50:\n",
    "                con_a = \"{} {}\".format(con_a_1, con_a_2)\n",
    "                conversations.append((con_a, con_b, con_a_2))\n",
    "            \n",
    "            con_a_1 = con_a_2\n",
    "    pickle.dump(conversations, open(os.path.join('data', 'conversations'), 'wb'), True)\n",
    "else:\n",
    "    conversations = pickle.load(open(os.path.join('data', 'conversations'), 'rb'))\n",
    "print(conversations[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize conversations and add padding ``<pad>``, ``<eos>``, ``<bos>`` and replace out of vocabulary words with ``<unk>``\n",
    "\n",
    "Maximum number of words in a sentence is 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 20)\n",
      "(15000, 20)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 20\n",
    "vocab = [w_v for w_v in word_index if word_index[w_v]< num_words]\n",
    "\n",
    "# These are not question and answers but a conversation. \n",
    "# Just for convenience sake, I used question and answer as variable names.\n",
    "question = []\n",
    "answer = []\n",
    "\n",
    "question_exists = os.path.exists(os.path.join('data', 'question'))\n",
    "answer_exists = os.path.exists(os.path.join('data', 'answer'))\n",
    "\n",
    "if not (question_exists & answer_exists):\n",
    "    for conv in conversations[:15000]:\n",
    "        conversation_a = conv[0]\n",
    "        conversation_b = conv[1]\n",
    "    \n",
    "        conversation_a = text_to_word_sequence(conversation_a)\n",
    "        conversation_b = text_to_word_sequence(conversation_b)\n",
    "    \n",
    "#         conversation_a.insert(0, '<bos>')\n",
    "#         conversation_a.append('<eos>')\n",
    "#         conversation_b.insert(0, '<bos>')\n",
    "#         conversation_b.append('<eos>')\n",
    "    \n",
    "        conversation_a = [word_to_index[c] if c in vocab else 3 for c in conversation_a]\n",
    "        conversation_b = [word_to_index[c] if c in vocab else 3 for c in conversation_b]\n",
    "        \n",
    "        question.append(conversation_a[:max_length])\n",
    "        answer.append(conversation_b[:max_length])\n",
    "    \n",
    "    question = pad_sequences(question, max_length, padding='pre')\n",
    "    answer = pad_sequences(answer, max_length, padding='post')\n",
    "    \n",
    "    pickle.dump(question, open(os.path.join('data', 'question'), 'wb'), True)\n",
    "    pickle.dump(answer, open(os.path.join('data', 'answer'), 'wb'), True)\n",
    "else:\n",
    "    question = pickle.load(open(os.path.join('data', 'question'), 'rb'))\n",
    "    answer = pickle.load(open(os.path.join('data', 'answer'), 'rb'))\n",
    "\n",
    "print(question.shape)\n",
    "print(answer.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_target = np.zeros((answer.shape[0], max_length, len(vocab) + 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 20, 8427)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "'i' format requires -2147483648 <= number <= 2147483647",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c1ee16303917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0manswer_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'answer_target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0manswer_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'answer_target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m     \u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROTO\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMARK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave_string\u001b[0;34m(self, obj, pack)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSHORT_BINSTRING\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBINSTRING\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTRING\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: 'i' format requires -2147483648 <= number <= 2147483647"
     ]
    }
   ],
   "source": [
    "# +4 for <eos>,<bos>, <unk> and <pad>\n",
    "# -1 in answer[k][j]-1 because fit_to_text assigns id from 1 to num_words \n",
    "# Here, we are setting 1 to the index of next word in the sentence because softmax \n",
    "# should predict that word as next word. So, high probability = 1.\n",
    "\n",
    "answer_target_exists = os.path.exists(os.path.join('data', 'answer_target'))\n",
    "if not answer_target_exists:\n",
    "    print(answer_target.shape)\n",
    "    for k in range(0, answer.shape[0]):\n",
    "        for j in range(0, answer.shape[1]):\n",
    "            if j > 0:\n",
    "                answer_target[k][j-1][answer[k][j]-1] = 1\n",
    "\n",
    "    pickle.dump(answer_target, open(os.path.join('data', 'answer_target'), 'wb'),True)\n",
    "else:\n",
    "    answer_target = pickle.load(open(os.path.join('data', 'answer_target'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Embedding Matrix shape', (8424, 100))\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(vocab) + 1, 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in vocab:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "print(\"Embedding Matrix shape\",embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=100\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "#Embedding(Size_Of_Vocab, Size_Of_Embedding_Vector, weights=[embedding_matrix] Input_Length)\n",
    "#Vocab_Size(rows)* embedding_vector_size(columns) must be equal to the embedding_matrix size) \n",
    "#enc_embedding_layer = Embedding(num_words+4, embedding_size,input_length=max_length)(encoder_inputs)\n",
    "#uncomment this line to include the embedding matrix\n",
    "\n",
    "enc_embedding_layer=  Embedding(num_words,embedding_size,weights=[embedding_matrix],input_length=max_length)(encoder_inputs)\n",
    "# enc_embedding_layer=  Embedding(num_words,embedding_size,input_length=max_length)(encoder_inputs)\n",
    "encoder = LSTM(100, return_state=True)\n",
    "_, state_h, state_c = encoder(enc_embedding_layer)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dex=Embedding(num_words,embedding_size, weights=[embedding_matrix])\n",
    "\n",
    "final_dex= dex(decoder_inputs)\n",
    "#uncomment this line to include the embedding_matrix\n",
    "#dec_embedding_layer = Embedding(55843, embedding_size,weights=[embedding_matrix],input_length=max_length)(decoder_inputs)\n",
    "decoder_lstm = LSTM(100, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex,initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(vocab) + 4, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14250 samples, validate on 750 samples\n",
      "Epoch 1/100\n",
      "14250/14250 [==============================] - 667s 47ms/step - loss: 0.0000e+00 - acc: 2.2105e-04 - val_loss: 0.0000e+00 - val_acc: 6.6667e-05\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.00007, saving model to weights.hdf5\n",
      "Epoch 2/100\n",
      " 8064/14250 [===============>..............] - ETA: 4:34 - loss: 0.0000e+00 - acc: 2.6662e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-145fd7b498ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m          callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shweta/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# parallel_model = multi_gpu_model(model, gpus=2)\n",
    "checkpointer = ModelCheckpoint(filepath = 'weights.hdf5', save_best_only=True, save_weights_only=True, verbose=1, monitor='val_acc')\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])\n",
    "model.fit([question, answer], answer_target,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "          validation_split=0.05,\n",
    "         callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Uncomment this if the training to the model is required\n",
    "encoder_input_data= conversation-dialogue 1 dataset\n",
    "decoder_input_data= conversation-dialogue 2 dataset\n",
    "decoder_target_data= conversation-dialogue 2 dataset\n",
    "\n",
    "Example: \n",
    "Creating the array of zeros with the required shape and populate these arrays with the data.\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(lines.eng), 7),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(lines.fr), 16),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(lines.fr), 16, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "   \n",
    "decoder_target_data is a 3 dimensional array, we need to construct a 3-dimensional array for the dense(softmax layer)\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(lines.eng, lines.fr)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        #decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        if t != 16:\n",
    "            decoder_input_data[i, t] = target_token_index[word]\n",
    "        if t > 0:\n",
    "            #decoder_target_data will be ahead by one timestep\n",
    "            #and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "\n",
    "model.fit([question, answer], decoder_target_data,batch_size=256,epochs=10000,validation_split=0.05)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "#In order to test or infer the model, encoder and decoder models are required.\n",
    "#As the encoder model gives the #encoder_states for a particular input which acts as an input to the decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 20, 100)           842400    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 100), (None, 100) 80400     \n",
      "=================================================================\n",
      "Total params: 922,800\n",
      "Trainable params: 922,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(embedding_size,))\n",
    "decoder_state_input_c = Input(shape=(embedding_size,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "final_dec_emb_layer= dex(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dec_emb_layer, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "#reverse_input_char_index = dict(\n",
    "#    (i, char) for char, i in input_token_index.items())\n",
    "# reverse_target_char_index = dict(\n",
    "#     (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import random\n",
    "# Different Sampling Strategies for Recurrent Neural Networks\n",
    "# Method for Beam Search\n",
    "def beam_search_decoder(data, k):\n",
    "    sequences = [[list(),1.0]]\n",
    "    all_candidates = list()\n",
    "    for i in range(len(sequences)):\n",
    "        seq,score = sequences[i]\n",
    "        for j in range(len(data)):\n",
    "            candidate = [seq + [j], score * -log(data[j])]\n",
    "            all_candidates.append(candidate)\n",
    "    ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "    sequences = ordered[:k]\n",
    "    x =  sequences[random.randint(0,( len(sequences)- 1))][0][0]\n",
    "    return x\n",
    "\n",
    "def beam_search_decoder_method2(data, k):\n",
    "    sequences = [[list(), 1.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score * -log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    result = sequences[random.randint(0, len(sequences))][0][0]\n",
    "    return result\n",
    "\n",
    "# Method for Random Sampling\n",
    "# try values [0.2, 0.5, 1.0, 1.2] for temperature\n",
    "def random_sample(predictions, temperature=1.0):\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    preds = np.exp(predictions)\n",
    "    predictions = preds / np.sum(preds)\n",
    "    probabilities = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probabilities)\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = word_to_index['<bos>']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        #sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        #sampled_token_index = beam_search_decoder(output_tokens[0, -1, :],10) #Calling beam search method\n",
    "        \n",
    "        sampled_token_index=random_sample(output_tokens[0, -1, :]) #Calling random sampling method\n",
    "        sampled_word = index_to_word[sampled_token_index]\n",
    "        \n",
    "        decoded_sentence += ' '+sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '<eos>' or\n",
    "           len(decoded_sentence) > max_length+2):   #added two for <bos> and <eos>\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "('Input', ['can', 'we', 'make', 'this', 'quick', '<unk>', '<unk>', 'and', 'andrew', '<unk>', 'are', 'having', 'an', 'incredibly', '<unk>', 'public', 'break', 'up', 'on', 'the'])\n",
      "('Decoded sentence:', ' publisher becomes salon')\n",
      "-\n",
      "('Input', ['can', 'we', 'make', 'this', 'quick', '<unk>', '<unk>', 'and', 'andrew', '<unk>', 'are', 'having', 'an', 'incredibly', '<unk>', 'public', 'break', 'up', 'on', 'the'])\n",
      "('Decoded sentence:', ' altogether following dennis')\n",
      "-\n",
      "('Input', ['well', 'i', 'thought', \"we'd\", 'start', 'with', '<unk>', 'if', \"that's\", 'okay', 'with', 'you', 'not', 'the', '<unk>', 'and', '<unk>', 'and', '<unk>', 'part'])\n",
      "('Decoded sentence:', \" annual popcorn where's\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"you're\", 'asking', 'me', 'out', \"that's\", 'so', 'cute', \"what's\", 'your', 'name', 'again'])\n",
      "('Decoded sentence:', ' fire entrance bodies posted')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'no', 'no', \"it's\", 'my', 'fault', 'we', \"didn't\", 'have', 'a', 'proper', '<unk>'])\n",
      "('Decoded sentence:', \" invite wood cia king's\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'no', 'no', \"it's\", 'my', 'fault', 'we', \"didn't\", 'have', 'a', 'proper', '<unk>', 'cameron'])\n",
      "('Decoded sentence:', ' grail understanding lead')\n",
      "-\n",
      "('Input', ['cameron', 'the', 'thing', 'is', 'cameron', \"i'm\", 'at', 'the', 'mercy', 'of', 'a', 'particularly', 'hideous', 'breed', 'of', 'loser', 'my', 'sister', 'i', \"can't\"])\n",
      "('Decoded sentence:', \" daily eleven rowan everyone's\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'why'])\n",
      "('Decoded sentence:', ' orphan finished listened')\n",
      "-\n",
      "('Input', ['why', '<unk>', 'mystery', 'she', 'used', 'to', 'be', 'really', 'popular', 'when', 'she', 'started', 'high', 'school', 'then', 'it', 'was', 'just', 'like', 'she'])\n",
      "('Decoded sentence:', ' ranger exit spite herself')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'gosh', 'if', 'only', 'we', 'could', 'find', 'kat', 'a', 'boyfriend'])\n",
      "('Decoded sentence:', ' taransky steven ounce brooklyn')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<unk>', 'ma', '<unk>', 'this', 'is', 'my', 'head'])\n",
      "('Decoded sentence:', ' sons woodly inspection')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<unk>', 'ma', '<unk>', 'this', 'is', 'my', 'head', 'right', 'see', \"you're\", 'ready', 'for', 'the', '<unk>'])\n",
      "('Decoded sentence:', ' mark scottie insulted foreign')\n",
      "-\n",
      "('Input', ['right', 'see', \"you're\", 'ready', 'for', 'the', '<unk>', 'i', \"don't\", 'want', 'to', 'know', 'how', 'to', 'say', 'that', 'though', 'i', 'want', 'to'])\n",
      "('Decoded sentence:', ' pray lose everywhere r')\n",
      "-\n",
      "('Input', ['i', \"don't\", 'want', 'to', 'know', 'how', 'to', 'say', 'that', 'though', 'i', 'want', 'to', 'know', 'useful', 'things', 'like', 'where', 'the', 'good'])\n",
      "('Decoded sentence:', \" jake beatin' eugene portland\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'how', 'is', 'our', 'little', 'find', 'the', '<unk>', 'a', 'date', 'plan', '<unk>'])\n",
      "('Decoded sentence:', ' colony spine bonus motherfucker')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'there'])\n",
      "('Decoded sentence:', ' win institution crown amazing')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'you', 'got', 'something', 'on', 'your', 'mind'])\n",
      "('Decoded sentence:', ' brody spats adopt scouts')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'you', 'have', 'my', 'word', 'as', 'a', 'gentleman'])\n",
      "('Decoded sentence:', ' twentieth miz foreign preserve')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'how', 'do', 'you', 'get', 'your', 'hair', 'to', 'look', 'like', 'that'])\n",
      "('Decoded sentence:', ' heavily contacts spotted')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'sure', 'have'])\n",
      "('Decoded sentence:', ' protect though store old')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', 'sure', 'have', 'i', 'really', 'really', 'really', 'wanna', 'go', 'but', 'i', \"can't\", 'not', 'unless', 'my', 'sister', 'goes'])\n",
      "('Decoded sentence:', ' thomas wendell salt frustrated')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"she's\", 'not', 'a'])\n",
      "('Decoded sentence:', ' bella ordell brings bean')\n",
      "-\n",
      "('Input', [\"she's\", 'not', 'a', 'lesbian', 'no', 'i', 'found', 'a', 'picture', 'of', '<unk>', '<unk>', 'in', 'one', 'of', 'her', '<unk>', 'so', \"i'm\", 'pretty'])\n",
      "('Decoded sentence:', ' jeep conscience cat science')\n",
      "-\n",
      "('Input', ['lesbian', 'no', 'i', 'found', 'a', 'picture', 'of', '<unk>', '<unk>', 'in', 'one', 'of', 'her', '<unk>', 'so', \"i'm\", 'pretty', 'sure', \"she's\", 'not'])\n",
      "('Decoded sentence:', ' lover what\\x92s engines prescription')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'hi'])\n",
      "('Decoded sentence:', \" drivin' 2 tribe submit\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'you', 'know', '<unk>'])\n",
      "('Decoded sentence:', \" cosmo who're roast talkin'\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'have', 'fun', 'tonight'])\n",
      "('Decoded sentence:', \" throat doin' responsibilities\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'i', 'looked', 'for', 'you', 'back', 'at', 'the', 'party', 'but', 'you', 'always', 'seemed', 'to', 'be', 'occupied'])\n",
      "('Decoded sentence:', ' classy were hiya marked')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', 'i', 'looked', 'for', 'you', 'back', 'at', 'the', 'party', 'but', 'you', 'always', 'seemed', 'to', 'be', 'occupied', 'i', 'was'])\n",
      "('Decoded sentence:', ' supposing trick pose treatment')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'well', 'no'])\n",
      "('Decoded sentence:', ' experiments conspiracy')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'well', 'no', 'then', \"that's\", 'all', 'you', 'had', 'to', 'say'])\n",
      "('Decoded sentence:', ' geek independence intellect')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'then', \"that's\", 'all', 'you', 'had', 'to', 'say', 'but'])\n",
      "('Decoded sentence:', ' desert couldn\\x92t examined')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', 'then', '<unk>', 'says', 'if', 'you', 'go', 'any', 'lighter', \"you're\", 'gonna', 'look', 'like', 'an', 'extra', 'on', '<unk>'])\n",
      "('Decoded sentence:', \" humanity genes askin' grace\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'do', 'you', 'listen', 'to', 'this', 'crap'])\n",
      "('Decoded sentence:', ' examination woke shuttle')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'do', 'you', 'listen', 'to', 'this', 'crap', 'what', 'crap'])\n",
      "('Decoded sentence:', ' behalf necessary fall benedict')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'what', 'crap', 'me', 'this', 'endless', 'blonde', '<unk>', \"i'm\", 'like', 'boring', 'myself'])\n",
      "('Decoded sentence:', ' warm wing jeremiah ding')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'i', 'figured', \"you'd\", 'get', 'to', 'the', 'good', 'stuff', 'eventually'])\n",
      "('Decoded sentence:', ' interviews half freezer')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'i', 'figured', \"you'd\", 'get', 'to', 'the', 'good', 'stuff', 'eventually', 'what', 'good', 'stuff'])\n",
      "('Decoded sentence:', ' coin drama fashioned joke')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'what', 'good', 'stuff', 'the', 'real', 'you'])\n",
      "('Decoded sentence:', ' range los goat spray watched')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "('Input', ['<pad>', '<pad>', \"i'm\", 'kidding', 'you', 'know', 'how', 'sometimes', 'you', 'just', 'become', 'this', '<unk>', 'and', 'you', \"don't\", 'know', 'how', 'to', 'quit'])\n",
      "('Decoded sentence:', ' dignan cause gwen symptoms')\n",
      "-\n",
      "('Input', ['<pad>', \"i'm\", 'kidding', 'you', 'know', 'how', 'sometimes', 'you', 'just', 'become', 'this', '<unk>', 'and', 'you', \"don't\", 'know', 'how', 'to', 'quit', 'no'])\n",
      "('Decoded sentence:', ' prominent beef driver mitch')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'wow'])\n",
      "('Decoded sentence:', ' limb philadelphia annie')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'she', 'okay'])\n",
      "('Decoded sentence:', ' susan embarrassed momma')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'they', 'do', 'to'])\n",
      "('Decoded sentence:', ' sofa doo sensible suits')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'did', 'you', 'change', 'your', 'hair'])\n",
      "('Decoded sentence:', ' also magician blind woods')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'did', 'you', 'change', 'your', 'hair', 'no'])\n",
      "('Decoded sentence:', ' satisfaction machines hold')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'where', 'did', 'he', 'go', 'he', 'was', 'just', 'here'])\n",
      "('Decoded sentence:', ' wallet listening sources')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'where', 'did', 'he', 'go', 'he', 'was', 'just', 'here', 'who'])\n",
      "('Decoded sentence:', ' lust family slapped girlfriend')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'great'])\n",
      "('Decoded sentence:', ' chancellor cole cleaner')\n",
      "-\n",
      "('Input', ['he', 'practically', 'proposed', 'when', 'he', 'found', 'out', 'we', 'had', 'the', 'same', '<unk>', 'i', 'mean', 'dr', '<unk>', 'is', 'great', 'an', 'all'])\n",
      "('Decoded sentence:', \" deeper where's orson bluestar\")\n",
      "-\n",
      "('Input', ['he', 'practically', 'proposed', 'when', 'he', 'found', 'out', 'we', 'had', 'the', 'same', '<unk>', 'i', 'mean', 'dr', '<unk>', 'is', 'great', 'an', 'all'])\n",
      "('Decoded sentence:', ' exchange keep sweating')\n",
      "-\n",
      "('Input', ['<pad>', 'bianca', 'i', \"don't\", 'think', 'the', '<unk>', 'of', 'dating', 'joey', '<unk>', 'are', 'going', 'to', 'include', 'door', 'opening', 'and', 'coat', 'holding'])\n",
      "('Decoded sentence:', ' harmony penelope colored')\n",
      "-\n",
      "('Input', ['bianca', 'i', \"don't\", 'think', 'the', '<unk>', 'of', 'dating', 'joey', '<unk>', 'are', 'going', 'to', 'include', 'door', 'opening', 'and', 'coat', 'holding', 'sometimes'])\n",
      "('Decoded sentence:', ' developing parents lawn')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'i', 'have', 'to', 'be', 'home', 'in', 'twenty', 'minutes'])\n",
      "('Decoded sentence:', ' flatter parrish hysterical')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'you', 'think', 'you', \"'\", 're', 'the', 'only', '<unk>', 'at', 'the', 'prom'])\n",
      "('Decoded sentence:', ' journey erase adults universe')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"it's\", 'more'])\n",
      "('Decoded sentence:', \" references 'n' carbon francs\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'exactly', 'so', 'you', 'going', 'to', '<unk>', '<unk>', 'thing', 'on', 'saturday'])\n",
      "('Decoded sentence:', ' automatic democratic cairo')\n",
      "-\n",
      "('Input', ['so', 'yeah', \"i've\", 'got', 'the', '<unk>', '<unk>', 'thing', 'going', 'and', 'the', 'tube', 'sock', 'gig', \"that's\", 'gonna', 'be', 'huge', 'and', 'then'])\n",
      "('Decoded sentence:', ' complaint waiting midnight')\n",
      "-\n",
      "('Input', ['so', 'yeah', \"i've\", 'got', 'the', '<unk>', '<unk>', 'thing', 'going', 'and', 'the', 'tube', 'sock', 'gig', \"that's\", 'gonna', 'be', 'huge', 'and', 'then'])\n",
      "('Decoded sentence:', \" stayin' hospitals ransom\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'neat'])\n",
      "('Decoded sentence:', ' obvious runner greatest')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'hey', 'sweet', 'cheeks'])\n",
      "('Decoded sentence:', ' necessary walk fellows')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'hey', 'sweet', 'cheeks', 'hi', 'joey'])\n",
      "('Decoded sentence:', ' critic stiff hate seduce')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'listen', 'i', 'want', 'to', 'talk', 'to', 'you', 'about', 'the', 'prom'])\n",
      "('Decoded sentence:', ' sire breathing tracking')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"where've\", 'you', 'been'])\n",
      "('Decoded sentence:', \" noir o'neil sleeps ergo\")\n",
      "-\n",
      "('Input', ['<pad>', 'i', 'have', 'the', 'potential', 'to', 'smack', 'the', 'crap', 'out', 'of', 'you', 'if', 'you', \"don't\", 'get', 'out', 'of', 'my', 'way'])\n",
      "('Decoded sentence:', ' pray hanna werewolf feeds')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'oh', 'my', 'god', 'does', 'this', 'mean', \"you're\", 'becoming', 'normal'])\n",
      "('Decoded sentence:', ' wizard effort fatal ages')\n",
      "-\n",
      "('Input', ['oh', 'my', 'god', 'does', 'this', 'mean', \"you're\", 'becoming', 'normal', 'it', 'means', 'that', '<unk>', 'is', 'playing', 'at', 'club', 'skunk', 'and', \"we're\"])\n",
      "('Decoded sentence:', ' episode horses drug raw')\n",
      "-\n",
      "('Input', ['it', 'means', 'that', '<unk>', 'is', 'playing', 'at', 'club', 'skunk', 'and', \"we're\", 'going', 'oh', 'i', 'thought', 'you', 'might', 'have', 'a', 'date'])\n",
      "('Decoded sentence:', ' georgie marvelous brought')\n",
      "-\n",
      "('Input', ['oh', 'i', 'thought', 'you', 'might', 'have', 'a', 'date', 'i', \"don't\", 'know', 'why', \"i'm\", 'bothering', 'to', 'ask', 'but', 'are', 'you', 'going'])\n",
      "('Decoded sentence:', ' proving marion lyssa awful')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"you're\", 'ruining', 'my', '<unk>', 'because', 'you', \"won't\", 'be', 'normal', 'i', \"can't\", 'be', 'normal'])\n",
      "('Decoded sentence:', ' colored fairly felson knocking')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"you're\", 'ruining', 'my', '<unk>', 'because', 'you', \"won't\", 'be', 'normal', 'i', \"can't\", 'be', 'normal', \"what's\", 'normal'])\n",
      "('Decoded sentence:', ' sexy harvard boatwright')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"can't\", 'you', 'forget', 'for', 'just', 'one', 'night', 'that', \"you're\", 'completely', 'wretched'])\n",
      "('Decoded sentence:', ' wade killed dea lips caesar')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'like', \"i'm\", 'supposed', 'to', 'know', 'what', 'that', 'even', 'means'])\n",
      "('Decoded sentence:', \" dated scott walkin' shot\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', 'like', \"i'm\", 'supposed', 'to', 'know', 'what', 'that', 'even', 'means', \"it's\", 'shakespeare', 'maybe', \"you've\", 'heard', 'of', 'him'])\n",
      "('Decoded sentence:', ' grandma experiencing years')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'you', 'are', 'so', 'completely', '<unk>'])\n",
      "('Decoded sentence:', ' ninth gardiner cord fog')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'bianca', 'i', 'need', 'to', 'talk', 'to', 'you', 'i', 'need', 'to', 'tell', 'you'])\n",
      "('Decoded sentence:', \" scent note o'brien busting\")\n",
      "-\n",
      "('Input', ['i', \"don't\", 'get', 'you', 'you', 'act', 'like', \"you're\", 'too', 'good', 'for', 'any', 'of', 'this', 'and', 'then', 'you', 'go', 'totally', '<unk>'])\n",
      "('Decoded sentence:', ' pimp chaos guests teachers')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'listen', 'i', 'know', 'you', 'hate', 'having', 'to', 'sit', 'home', 'because', \"i'm\", 'not', 'susie', 'high', 'school'])\n",
      "('Decoded sentence:', ' inspired belly adopt bait')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "('Input', ['<pad>', '<pad>', 'listen', 'i', 'know', 'you', 'hate', 'having', 'to', 'sit', 'home', 'because', \"i'm\", 'not', 'susie', 'high', 'school', 'like', 'you', 'care'])\n",
      "('Decoded sentence:', ' seats pockets spells serve')\n",
      "-\n",
      "('Input', ['like', 'you', 'care', 'i', 'do', 'care', 'but', \"i'm\", 'a', 'firm', '<unk>', 'in', 'doing', 'something', 'for', 'your', 'own', 'reasons', 'not', 'someone'])\n",
      "('Decoded sentence:', ' those communists flattered')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'joey', 'never', 'told', 'you', 'we', 'went', 'out', 'did', 'he'])\n",
      "('Decoded sentence:', ' ease dolores united whatcha')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'joey', 'never', 'told', 'you', 'we', 'went', 'out', 'did', 'he', 'what'])\n",
      "('Decoded sentence:', \" resistance fame drivin'\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'what', 'in', '<unk>', 'for', 'a', 'month'])\n",
      "('Decoded sentence:', ' nuclear vision nearer tire')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'in', '<unk>', 'for', 'a', 'month', 'why'])\n",
      "('Decoded sentence:', ' penny past passing serving')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'why', 'he', 'was', 'like', 'a', 'total', 'babe'])\n",
      "('Decoded sentence:', ' habit philosophy bonaparte')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'he', 'was', 'like', 'a', 'total', 'babe', 'but', 'you', 'hate', 'joey'])\n",
      "('Decoded sentence:', ' mum thirteen sicily sometime')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'but', 'you', 'hate', 'joey', 'now', 'i', 'do', 'back', 'then', 'was', 'a', 'different', 'story'])\n",
      "('Decoded sentence:', ' races johnny confession')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'he', 'said', 'everyone', 'was', 'doing', 'it', 'so', 'i', 'did', 'it'])\n",
      "('Decoded sentence:', \" costume nor mark anybody's\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'he', 'said', 'everyone', 'was', 'doing', 'it', 'so', 'i', 'did', 'it', 'you', 'did', 'what'])\n",
      "('Decoded sentence:', ' suckers audrey hard shakespeare')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'but'])\n",
      "('Decoded sentence:', ' court limit tran sparrow')\n",
      "-\n",
      "('Input', ['but', 'after', 'that', 'i', 'swore', \"i'd\", 'never', 'do', 'anything', 'just', 'because', 'everyone', 'else', 'was', 'doing', 'it', 'and', 'i', \"haven't\", 'since'])\n",
      "('Decoded sentence:', \" molecular driver's shotgun\")\n",
      "-\n",
      "('Input', ['after', 'that', 'i', 'swore', \"i'd\", 'never', 'do', 'anything', 'just', 'because', 'everyone', 'else', 'was', 'doing', 'it', 'and', 'i', \"haven't\", 'since', 'except'])\n",
      "('Decoded sentence:', ' blows soup bourne maximum')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', 'why', \"didn't\", 'you', 'tell', 'me', 'i', 'wanted', 'to', 'let', 'you', 'make', 'up', 'your', 'own', 'mind', 'about', 'him'])\n",
      "('Decoded sentence:', ' absurd arrange video slip')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"that's\", 'not'])\n",
      "('Decoded sentence:', ' trick apology anne ni choir')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \"that's\", 'not', \"i'm\", 'not', 'stupid', 'enough', 'to', 'repeat', 'your', 'mistakes'])\n",
      "('Decoded sentence:', \" collecting slim dyin' transportation\")\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', \"i'm\", 'not', 'stupid', 'enough', 'to', 'repeat', 'your', 'mistakes', 'i', 'guess', 'i', 'thought', 'i', 'was', 'protecting', 'you'])\n",
      "('Decoded sentence:', ' innocence wrap does torrance')\n",
      "-\n",
      "('Input', ['i', 'guess', 'i', 'thought', 'i', 'was', 'protecting', 'you', 'god', \"you're\", 'just', 'like', 'him', 'just', 'keep', 'me', 'locked', 'away', 'in', 'the'])\n",
      "('Decoded sentence:', ' hustle brat should freedom')\n",
      "-\n",
      "('Input', ['god', \"you're\", 'just', 'like', 'him', 'just', 'keep', 'me', 'locked', 'away', 'in', 'the', 'dark', 'so', 'i', \"can't\", 'experience', 'anything', 'for', 'myself'])\n",
      "('Decoded sentence:', ' face maintenance cheese')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'you', 'looked', 'beautiful', 'last', 'night', 'you', 'know'])\n",
      "('Decoded sentence:', ' soldiers compassion remembered')\n",
      "-\n",
      "('Input', ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'let', 'go'])\n",
      "('Decoded sentence:', \" causing medieval kiddin'\")\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(0, 100):\n",
    "    input_seq = question[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "#     print('Input sentence:', question[seq_index: seq_index + 1])\n",
    "    print('Input', [index_to_word[k] for k in input_seq[0]])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
